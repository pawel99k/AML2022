{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder ### available? Imho should be, like cmon\n",
    "from sklearn.model_selection import train_test_split ###hopefully we're allowed to use this too\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18f74e",
   "metadata": {},
   "source": [
    "Function to remove features from the dataset basing on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeleteCorrelated(X,thresh=0.75):\n",
    "    X=X.copy()\n",
    "    cor_matrix = X.corr().abs()\n",
    "    upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] >= thresh)]\n",
    "    X_cleaned=X.drop(columns=to_drop)\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461e999",
   "metadata": {},
   "source": [
    "class to remove features from the dataset basing on Variance Importance Factor (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b43716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.kaggle.com/remilpm/how-to-remove-multicollinearity\n",
    "\n",
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=5, impute=True, impute_strategy='median'):\n",
    "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
    "        # Above 10 is too high and so should be removed.\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = SimpleImputer(strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'imputer'):\n",
    "            self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'imputer'):\n",
    "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh):\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            variables = X.columns\n",
    "            dropped = False\n",
    "            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "            \n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        print(X.shape[1],\" features left in dataset\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_test_split(X, y, train_subset_proportion=0.75, keep_y_balance=True):\n",
    "        if set(X.index) != set(y.index):\n",
    "            raise AttributeError('Indices in X and y are not indetical')\n",
    "        n=X.shape[0]\n",
    "        train_rows_n = int(train_subset_proportion * n)\n",
    "        test_rows_n = n - train_rows_n \n",
    "        if keep_y_balance:\n",
    "            if ((y.unique()!=0) & (y.unique()!=1)).any():\n",
    "                raise ValueError('Using keep_y_balance requires y values to be 0 and 1.')\n",
    "            pos_index = y[y==1].index\n",
    "            neg_index = y[y==0].index\n",
    "            train_pos_index = np.random.choice(pos_index, int(train_subset_proportion*len(pos_index)), replace=False)\n",
    "            train_neg_index = np.random.choice(neg_index, int(train_subset_proportion*len(neg_index)), replace=False)\n",
    "            test_pos_index = np.array(list(set(pos_index) - set(train_pos_index)))\n",
    "            test_neg_index = np.array(list(set(neg_index) - set(train_neg_index)))\n",
    "            train_index = np.concatenate((train_pos_index, train_neg_index))\n",
    "            test_index = np.concatenate((test_pos_index, test_neg_index))\n",
    "        else:\n",
    "            train_index = np.random.choice(y.index, train_rows_n, replace=False)\n",
    "            test_index = np.array(set(y.index) - set(train_index))\n",
    "        return X.loc[train_index, :], X.loc[test_index, :], y.loc[train_index], y.loc[test_index]\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_multicollinearity(X):\n",
    "        \"\"\"\n",
    "        https://stackoverflow.com/questions/25676145/capturing-high-multi-collinearity-in-statsmodels\n",
    "        https://en.wikipedia.org/wiki/Multicollinearity#Detection\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        while True:\n",
    "            corr_m = np.corrcoef(X)\n",
    "            eigenvalues, eigenvectors = np.linalg.eig(corr_m)\n",
    "            #TODO\n",
    "            print(eigenvalues)\n",
    "            break\n",
    "    \n",
    "    def one_hot_encoding(self):\n",
    "        #TODO\n",
    "        pass        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-deployment",
   "metadata": {},
   "source": [
    "# 1. Bank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
    "bank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in bank_df.columns:\n",
    "    if bank_df[c].dtype=='O':\n",
    "        print(c,bank_df[c].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-sterling",
   "metadata": {},
   "source": [
    "At first, it looks like there are no missing values in data. However, most off the categorical variables do have a special value `unknown` which is actually a missing value. While transforming the data by OHE, we can treat it like any other value or simply drop it to keep the columns linear independence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-symposium",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_bank = bank_df['y']=='no'\n",
    "bank_df.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a77039",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bank_raw_categorical=bank_df.drop(columns='y').select_dtypes(include='object')\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "encoder.fit(X_bank_raw_categorical)\n",
    "X_bank_cat_enc=encoder.transform(X_bank_raw_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919d400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_bank_numerical=bank_df.select_dtypes(exclude='object')\n",
    "Mult_Coll=ReduceVIF()\n",
    "X_bank_numerical = Mult_Coll.fit_transform(X_bank_numerical)\n",
    "X_bank_enc=X_bank_numerical.join(pd.DataFrame(X_bank_cat_enc,columns=np.concatenate(encoder.categories_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_bank_enc,y_bank,test_size=0.25)\n",
    "\n",
    "model2 = LogisticRegression(max_iter=1e20)\n",
    "model2.fit(X_train, y_train)\n",
    "print(X_train.shape[1])\n",
    "abs(model2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, y1, y2 = Preprocessor.train_test_split(bank_df.drop(columns='y'), bank_df['y']=='yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-stanford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094019b",
   "metadata": {},
   "source": [
    "# 2. Diabetic Retinopathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67eb22d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "data = arff.loadarff('data/messidor_features.arff')\n",
    "retinopathy_df = pd.DataFrame(data[0])\n",
    "retinopathy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "retinopathy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b94b3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for c in retinopathy_df.columns:\n",
    "    print(\"column name: \",c,\"number of unique values: \",len(retinopathy_df[c].unique()),\"number of nulls\",retinopathy_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ad9c0",
   "metadata": {},
   "source": [
    "description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retinopathy_df.drop(columns='Class',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_retinopathy=retinopathy_df.drop(columns='18')\n",
    "y_retinopathy=retinopathy_df['18']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bae49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_retinopathy.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6a06d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Mult_Coll = ReduceVIF()\n",
    "\n",
    "X_retinopathy_clean = Mult_Coll.fit_transform(X_retinopathy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b46e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_retinopathy_clean.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb21c7",
   "metadata": {},
   "source": [
    "#### Results without VIF reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_retinopathy,y_retinopathy,test_size=0.2)\n",
    "\n",
    "model = LogisticRegression(max_iter=1e20)\n",
    "model.fit(X_train, y_train)\n",
    "print(X_train.shape[1])\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e0ca2",
   "metadata": {},
   "source": [
    "#### Results with VIF reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fae9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_retinopathy_clean,y_retinopathy,test_size=0.2)\n",
    "\n",
    "model = LogisticRegression(max_iter=1e20)\n",
    "model.fit(X_train, y_train)\n",
    "print(X_train.shape[1])\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1842b6",
   "metadata": {},
   "source": [
    "#### Results with feature selection based on correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_corr=DeleteCorrelated(X_retinopathy)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_no_corr,y_retinopathy,test_size=0.2)\n",
    "\n",
    "model = LogisticRegression(max_iter=1e20)\n",
    "model.fit(X_train, y_train)\n",
    "print(X_train.shape[1])\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ab354",
   "metadata": {},
   "source": [
    "# 3. Breast Cancer Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdbc_df=pd.read_csv('data/wdbc.csv')\n",
    "\n",
    "wdbc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465aa873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_wdbc=wdbc_df['diagnosis']==\"M\"\n",
    "X_wdbc=wdbc_df.drop(columns=[\"id\",\"diagnosis\",\"Unnamed: 32\"])\n",
    "\n",
    "X_wdbc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c956f",
   "metadata": {},
   "source": [
    "As we can see all features are non-null numeric type variables. Which means that in this case one-hot-encoding won't be needed. The only things left to do is to remove collinear and multicollinear ones (maybe remove some outliers? from data) and split data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75487cb6",
   "metadata": {},
   "source": [
    "Correlation matrix showing that we should probably remove a fair number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6757e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_wdbc.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffdc56",
   "metadata": {},
   "source": [
    "Removal of variables based only on correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wdbc_cleaned_corr=DeleteCorrelated(X_wdbc,0.8)\n",
    "X_wdbc_cleaned_corr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23d06a",
   "metadata": {},
   "source": [
    "Removal of variables using Variance Inflation Factor (VIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mult_Coll = ReduceVIF()\n",
    "X_wdbc_cleaned = Mult_Coll.fit_transform(X_wdbc)\n",
    "X_wdbc_cleaned.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_wdbc_cleaned,y_wdbc,test_size=0.2)\n",
    "\n",
    "model = LogisticRegression(max_iter=1e20)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479426d6",
   "metadata": {},
   "source": [
    "## 4. Etherneum frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a184a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etherneum_df=pd.read_csv('data/transaction_dataset.csv')\n",
    "\n",
    "etherneum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275090bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in etherneum_df.columns:\n",
    "    if(len(etherneum_df[c].unique())<10):\n",
    "        print(c,etherneum_df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46c6ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etherneum_df[' ERC20 uniq sent addr.1'].fillna(0)\n",
    "etherneum_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e3538",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop=['Unnamed: 0',\n",
    "         'Index',\n",
    "         'Address',\n",
    "         ' ERC20 avg time between sent tnx',\n",
    "         ' ERC20 avg time between rec tnx',\n",
    "         ' ERC20 avg time between rec 2 tnx',\n",
    "         ' ERC20 avg time between contract tnx',\n",
    "         ' ERC20 min val sent contract',\n",
    "         ' ERC20 max val sent contract',\n",
    "         ' ERC20 avg val sent contract']\n",
    "etherneum_df.drop(columns=to_drop,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e821a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in etherneum_df.columns:\n",
    "    if etherneum_df[c].dtype=='O':\n",
    "        print(c,len(etherneum_df[c].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eth=etherneum_df['FLAG']\n",
    "X_eth_raw=etherneum_df.drop(columns='FLAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b4e90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_eth_raw.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019b94a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Mult_Coll = ReduceVIF(thresh=7)\n",
    "X_eth_cleaned = Mult_Coll.fit_transform(X_eth_raw.drop(columns=[' ERC20_most_rec_token_type',' ERC20 most sent token type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d4df1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,13))\n",
    "sns.heatmap(X_eth_cleaned.corr())\n",
    "plt.show()\n",
    "#one could still consider droping some of the variables because of high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2dc2e8",
   "metadata": {},
   "source": [
    "#### Results when using only numerical features\n",
    "without VIF reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b6b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eth_no_categorical=X_eth_raw.drop(columns=[' ERC20 most sent token type',' ERC20_most_rec_token_type']).fillna(0)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_eth_no_categorical,\n",
    "                                                  y_eth,\n",
    "                                                  test_size=0.2\n",
    "                                                 )\n",
    "\n",
    "model0 = LogisticRegression(max_iter=1e20)\n",
    "model0.fit(X_train, y_train)\n",
    "print(X_train.shape[1])\n",
    "model0.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e273bab",
   "metadata": {},
   "source": [
    "#### Results when using only numerical features\n",
    "with VIF reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda968a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_eth_cleaned,y_eth,test_size=0.2)\n",
    "\n",
    "model1 = LogisticRegression(max_iter=1e20)\n",
    "model1.fit(X_train, y_train)\n",
    "print(X_eth_cleaned.shape[1])\n",
    "model1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc25e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_eth_raw_categorical=X_eth_raw[[' ERC20 most sent token type',' ERC20_most_rec_token_type']].fillna(\"unknown\")\n",
    "encoder=OneHotEncoder(sparse=False)\n",
    "encoder.fit(X_eth_raw_categorical)\n",
    "X_eth_cat_enc=encoder.transform(X_eth_raw_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1c3f0",
   "metadata": {},
   "source": [
    "ciekawostka:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67719e5e",
   "metadata": {},
   "source": [
    "#### Results when using only categorical features encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(pd.DataFrame(X_eth_cat_enc,columns=np.concatenate(encoder.categories_)),\n",
    "                                                  y_eth,\n",
    "                                                  test_size=0.25\n",
    "                                                 )\n",
    "\n",
    "model2 = LogisticRegression(max_iter=1e20)\n",
    "model2.fit(X_train, y_train)\n",
    "print(X_train.shape[1])\n",
    "abs(model2.score(X_test, y_test)-model2.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241fb3d",
   "metadata": {},
   "source": [
    "#### Results with categorical features encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67627fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eth_enc = X_eth_cleaned.join(pd.DataFrame(X_eth_cat_enc.toarray(),columns=np.concatenate(encoder.categories_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e988744",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X_eth_enc,y_eth,test_size=0.2)\n",
    "\n",
    "model2 = LogisticRegression(max_iter=1e20)\n",
    "model2.fit(X_train, y_train)\n",
    "print(X_train.shape[1])\n",
    "abs(model2.score(X_test, y_test)-model2.score(X_train, y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
